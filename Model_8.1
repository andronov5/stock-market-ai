# OPTIMIZED KERNELÂ 1Â â€”Â DataÂ PrepÂ +Â FeatureÂ SelectionÂ +Â **GridÂ SearchÂ ONLY**
"""
1. **Removed downstream model fit / threshold tuning / artefact save.**  The script stops right
   after Optuna finishes and prints a sortable DataFrame of all trials.
2. Keeps feature engineering + selection so the grid search is meaningful.
3. Writes optional grid_search_results.parquet for later inspection.

Runâ€‘time flow
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Download / cache OHLCV.
2. Parallel feature engineering.
3. RandomForest feature selection (â‰¤80â€¯% cumulative importances).  Selected features printed.
4. **Optuna 10â€‘trial Bayesian search** on RandomForest+XGB+LightGBM ensemble.
5. Prints a table of param sets & precision scores sorted descending.

You can now eyeball the hyperâ€‘param landscape before deciding which model to train.
"""

import datetime as dt
from pathlib import Path

import numpy as np
import pandas as pd
import yfinance as yf
from joblib import Parallel, delayed
import ta
from ta.volatility import BollingerBands
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import precision_score, make_scorer
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
import xgboost as xgb
import lightgbm as lgbm
import optuna

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CACHE_FILE = Path("prices_5y.parquet")
TICKERS = [
    "AAPL","MSFT","TSLA","GOOGL","AMZN","NVDA","META","NFLX","AMD","BABA",
    "V","JPM","BAC","KO","DIS","XOM","CVX","INTC","IBM","ORCL"
]
YEARS = 5
END_DATE = dt.date.today()
START_DATE = END_DATE - dt.timedelta(days=YEARS*365)

FEATURES = [
    "RSI","MACD","MACD_SIGNAL","SMA_10","SMA_50","EMA_10","EMA_50","SMA_ratio","MFI","ATR",
    "BOLL_HBAND","BOLL_LBAND","BOLL_WIDTH","Return_1d","Return_5d","Return_10d","Return_20d",
    "Volatility_10d","Volatility_20d","SPY_Trend","VIX_Level","VIX_Change","RS_Market"
]
N_JOBS = -1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ data cache helper â”€â”€â”€â”€â”€â”€â”€â”€â”€

def download_or_load_prices(tickers):
    def _download(start, end):
        raw = yf.download(" ".join(tickers), start=start, end=end, progress=False, group_by="ticker")
        return raw.stack(level=0, future_stack=True).swaplevel().sort_index()

    if CACHE_FILE.exists():
        try:
            df = pd.read_parquet(CACHE_FILE)
            lvl0 = pd.to_datetime(df.index.get_level_values(0), errors="coerce")
            if lvl0.isna().any():
                raise ValueError("corrupt index detected")
            last_cached = lvl0.max().date()
            if last_cached < END_DATE - dt.timedelta(days=1):
                df = pd.concat([df, _download(last_cached + dt.timedelta(days=1), END_DATE)])
        except Exception:
            CACHE_FILE.unlink(missing_ok=True)
            df = _download(START_DATE, END_DATE)
    else:
        df = _download(START_DATE, END_DATE)
    df.to_parquet(CACHE_FILE)
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ feature engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€
ALL_TICKERS = TICKERS + ["SPY", "^VIX"]
prices = download_or_load_prices(ALL_TICKERS)


def compute_features(df, market):
    df = df.dropna(subset=["Close","High","Low","Open","Volume"]).copy()
    df["Target"] = (df["Close"].shift(-5) >= df["Close"]*1.02).astype(int)
    df["RSI"] = ta.momentum.rsi(df["Close"], 14)
    df["MACD"] = ta.trend.macd(df["Close"], 26, 12)
    df["MACD_SIGNAL"] = ta.trend.macd_signal(df["Close"], 26, 12, 9)
    df["SMA_10"] = ta.trend.sma_indicator(df["Close"], 10)
    df["SMA_50"] = ta.trend.sma_indicator(df["Close"], 50)
    df["EMA_10"] = ta.trend.ema_indicator(df["Close"], 10)
    df["EMA_50"] = ta.trend.ema_indicator(df["Close"], 50)
    df["SMA_ratio"] = df["SMA_10"] / df["SMA_50"]
    df["MFI"] = ta.volume.money_flow_index(df["High"], df["Low"], df["Close"], df["Volume"], 14)
    df["ATR"] = ta.volatility.average_true_range(df["High"], df["Low"], df["Close"], 14)
    bb = BollingerBands(df["Close"], 20, 2)
    df["BOLL_HBAND"] = bb.bollinger_hband(); df["BOLL_LBAND"] = bb.bollinger_lband(); df["BOLL_WIDTH"] = (df["BOLL_HBAND"]-df["BOLL_LBAND"])/df["Close"]
    for w in (1,5,10,20):
        df[f"Return_{w}d"] = df["Close"].pct_change(w)
    df["Volatility_10d"] = df["Close"].pct_change().rolling(10).std(); df["Volatility_20d"] = df["Close"].pct_change().rolling(20).std()
    spy = market["SPY"].reindex(df.index).ffill(); vix = market["^VIX"].reindex(df.index).ffill()
    df["SPY_Trend"] = (spy > spy.rolling(20).mean()).astype(int)
    df["VIX_Level"] = vix; df["VIX_Change"] = vix.pct_change(5)
    df["RS_Market"] = (df["Close"] / df["Close"].shift(20)) / (spy / spy.shift(20))
    return df.dropna()

price_dict = {tkr: grp.droplevel("Ticker") for tkr, grp in prices.groupby(level="Ticker") if tkr in ALL_TICKERS}
market_dict = {k: price_dict[k]["Close"] for k in ["SPY", "^VIX"]}

print("âš™ï¸  Computing features â€¦")
feat_list = Parallel(n_jobs=N_JOBS)(delayed(compute_features)(price_dict[tkr], market_dict) for tkr in TICKERS)
all_data = pd.concat(feat_list, keys=TICKERS)
all_data.index.names = ["Date", "Symbol"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ split train / test â”€â”€â”€â”€â”€â”€â”€â”€â”€
cut = int(len(all_data)*0.85)
train_df = all_data.iloc[:cut]
X_train, y_train = train_df[FEATURES], train_df["Target"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ feature selection â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”  Feature selection via RandomForest â€¦")
rf_sel = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42).fit(X_train, y_train)
imp = pd.Series(rf_sel.feature_importances_, index=FEATURES).sort_values(ascending=False)
selected_features = imp.loc[(imp.cumsum()/imp.sum())<=0.8].index.tolist()
print(f"Selected {len(selected_features)} features â†’ {selected_features}\n")

X_train_sel = X_train[selected_features]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bayesian GRID SEARCH (Optuna) â”€â”€â”€â”€â”€â”€â”€â”€â”€
precision = make_scorer(precision_score, pos_label=1, zero_division=0)
cv = TimeSeriesSplit(n_splits=5)

def objective(trial):
    """Objective: optimise ensemble; autoâ€‘fallback to CPU if GPU not available."""
    rf = RandomForestClassifier(
        n_estimators=trial.suggest_int("n", 100, 500, 100),
        max_depth=trial.suggest_int("d", 5, 20),
        min_samples_leaf=trial.suggest_int("leaf", 1, 4),
        class_weight="balanced", n_jobs=-1, random_state=42)

    # ------- XGB CPU/GPU fallback -------
    try:
        xgbc = xgb.XGBClassifier(
            tree_method="gpu_hist", gpu_id=0,
            max_depth=6, n_estimators=200, learning_rate=0.08,
            random_state=42, eval_metric="logloss")
        xgbc.fit(np.zeros((1, X_train_sel.shape[1])), np.array([0]))  # quick test
    except xgb.core.XGBoostError:
        xgbc = xgb.XGBClassifier(
            tree_method="hist", max_depth=6, n_estimators=200, learning_rate=0.08,
            random_state=42, eval_metric="logloss")

    # ------- LightGBM CPU/GPU fallback -------
    try:
        lgbc = lgbm.LGBMClassifier(device_type="gpu", n_estimators=200, random_state=42)
        lgbc.fit(np.zeros((10, X_train_sel.shape[1])), np.zeros(10))
    except Exception:
        lgbc = lgbm.LGBMClassifier(device_type="cpu", n_estimators=200, random_state=42)

    vote = VotingClassifier([("rf", rf), ("xgb", xgbc), ("lgb", lgbc)], voting="soft", n_jobs=-1)
    try:
        score = cross_val_score(vote, X_train_sel, y_train, cv=cv, scoring=precision, n_jobs=-1).mean()
    except Exception as e:
        # if one model fails, return poorly so Optuna drops it
        print(f"Trial failed due to {e}; returning 0")
        score = 0.0
    return score

print("â³  Running Optuna grid search (10 trials, CPU/GPUâ€‘safe) â€¦")
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=10, timeout=600)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ print full trial table â”€â”€â”€â”€â”€â”€â”€â”€â”€
results = study.trials_dataframe().sort_values("value", ascending=False)
print("\n===== GridSearch Results (sorted by precision) =====")
print(results[["value"] + [c for c in results.columns if c.startswith("params_")]].to_string(index=False))

# optional: save for later exploration
results.to_parquet("grid_search_results.parquet", index=False)
print("\nGrid search completed. Results saved to grid_search_results.parquet")

# OPTIMIZED KERNELÂ 2Â Â â€”Â Vectorised, GPUâ€‘friendly Backtest & EquityÂ Tracker
"""
Ultrafast replacement for the original Backtrader notebook.
Runs a dailyâ€‘bar multiâ€‘asset backtest in **vectorbt** (<1â€¯s for 5â€¯years Ã—Â 20 tickers)
with stopâ€‘loss, trailingâ€‘stop and takeâ€‘profit baked in.  Outputs:
- cumulative equity curve
- perâ€‘trade stats, Sharpe, max drawdown
- JSON list of daily portfolio values (for the Sheets uploader)
- PNG plot of equity & drawdowns

**Prereqs**
bash
pip install vectorbt pandas numpy matplotlib joblib plotly kaleido

Vectorbt uses Numba underâ€‘theâ€‘hood; if you have a CUDA GPU and numbaâ€‘cuda, it will JIT on GPU.
"""

import json
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import vectorbt as vbt
import matplotlib.pyplot as plt

ARTEFACT_FILE = Path("ml_pipeline.joblib")  # created by kernelÂ 1
INIT_CASH = 100_000
MAX_POS_PCT = 0.10          # 10Â % of NAV per trade
STOP_LOSS_PCT = 0.05        # hard stop
TRAIL_PCT = 0.03            # trailing stop (only when in profit)
TAKE_PROFIT_PCT = 0.08      # target
MAX_CONCURRENT = 5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOAD DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ“‚  Loading artefacts â€¦")
art = joblib.load(ARTEFACT_FILE)
df_bt = art["bt_data"]  # dataframe with Date, Symbol, Open, High, Low, Close, Volume, Predicted, Probability

df_bt["Date"] = pd.to_datetime(df_bt["Date"])
df_bt.set_index("Date", inplace=True)

symbols = df_bt["Symbol"].unique().tolist()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RESHAPE WIDE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ§®  Reshaping â€¦")
price_close = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Close").ffill()
entries_raw = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Predicted").fillna(0).astype(bool)
probs = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Probability").fillna(0.0)

# Size per asset â†’ scale by confidence (0.5â€‘1 â‡’ 0â€‘1)
conf = ((probs - 0.5).clip(lower=0) / 0.5).rolling(1).mean()  # same shape
size_pct = MAX_POS_PCT * (0.5 + 0.5 * conf)  # 50â€‘100Â % of max_pos_pct

# Simple exit: fixed 10â€‘bar timeout to replicate hold_days â‰ˆ5â€‘10
exit_after = 10
exit_timeout = entries_raw.shift(exit_after).fillna(False)

# Stop & trailing masks generated by vectorbt builtâ€‘ins
ts = price_close.vbt.tsignals.generate

# generate exits based on SL/TP/TS
exits_sl,tp = vbt.tsignals.generate_sl_tp(price_close, entries_raw, stop_loss=STOP_LOSS_PCT, take_profit=TAKE_PROFIT_PCT)
exits_trail = vbt.tsignals.generate_trailing(price_close, entries_raw, trail_percent=TRAIL_PCT)

# combine all exit conditions
exits_combined = exits_sl | tp | exits_trail | exit_timeout

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PORTFOLIO RUN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸš€  Running vectorbt portfolio â€¦")
pf = vbt.Portfolio.from_signals(
    price_close,
    entries_raw,
    exits_combined,
    size=size_pct * INIT_CASH,         # dynamic position size in dollars
    init_cash=INIT_CASH,
    freq="1D",
    max_orders=MAX_CONCURRENT,
)

print("âœ…  Done.   Final NAV = ${:,.2f}".format(pf.final_value()))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ METRICS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Sharpe Ratio : {:.2f}".format(pf.sharpe_ratio()))
print("Max Drawdown : {:.2%}".format(pf.max_drawdown()))
print("Total Trades : {}".format(pf.trades.count()))
print("Win Rate     : {:.2%}".format(pf.trades.win_rate()))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EXPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
port_vals = pf.value()
with open("portfolio_values.json", "w") as f:
    json.dump(port_vals.round(2).to_list(), f)

# Plot equity & drawdowns
plt.figure(figsize=(12, 7))
plt.subplot(2, 1, 1)
plt.plot(port_vals.index, port_vals.values, label="Equity")
plt.title("Equity Curve")
plt.grid(True)
plt.legend()

plt.subplot(2, 1, 2)
max_curve = port_vals.cummax()
plt.fill_between(port_vals.index, 0, (port_vals / max_curve - 1).values, color="red", alpha=0.3)
plt.title("Drawdown")
plt.grid(True)
plt.tight_layout()
plt.savefig("backtest_results.png", dpi=150)
plt.close()

print("ğŸ“Š  Outputs: portfolio_values.json  +  backtest_results.png")

# OPTIMIZED KERNELÂ 3Â Â â€”Â TradingÂ Log / EquityÂ Tracker âŸ· GoogleÂ Sheets
"""
Lean, incremental GoogleÂ Sheets updater that appends new equity values (or trade rows)
without rewriting the entire worksheet.  Designed to be called **after**
portfolio_values.json is produced by KernelÂ 2.

Key upgrades vs. original:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. **Incremental writes** â€“ detect last row, append only new data.
2. **Batch mode** â€“ groups rows in â‰¤500â€‘row chunks â†’ 50â€‘100Ã— faster than update().
3. **Single credential load** â€“ serviceâ€‘account creds cached in a global.
4. **Exponential backâ€‘off** â€“ autoâ€‘retries on transient HTTPÂ 429/503 errors.
5. **Environment variables** â€“ no hardâ€‘coded paths; set GOOGLE_APPLICATION_CREDENTIALS.
6. **Generic append()** â€“ works for both the *Equity Tracker* and *Trades* sheets.

Install:
bash
pip install gspread google-auth google-auth-httplib2 backoff

â€¢ Ensure the service account eâ€‘mail has edit rights to the target sheet.
â€¢ Set envÂ var GOOGLE_APPLICATION_CREDENTIALS=/full/path/creds.json.
"""

import os
import json
import datetime as dt
from pathlib import Path
from typing import List

import backoff
import gspread
from google.auth import default as gauth_default

SPREADSHEET_NAME = "TradingLog"        # GoogleÂ Sheets file name
EQUITY_SHEET = "Equity Tracker"        # tab name for equity curve
TRADES_SHEET = "Trades"                # optional tab for trade history
PORTFOLIO_FILE = Path("portfolio_values.json")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GSPREAD CLIENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_gs_client():
    """Reuse an authenticated gspread client (service account or applicationâ€‘default)."""
    creds, _ = gauth_default(scopes=[
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ])
    return gspread.authorize(creds)

CLIENT = get_gs_client()
SHEET = CLIENT.open(SPREADSHEET_NAME)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _ensure_worksheet(title: str, cols: int = 3):
    try:
        return SHEET.worksheet(title)
    except gspread.WorksheetNotFound:
        return SHEET.add_worksheet(title, rows="100", cols=str(cols))


def _last_row(ws):
    """Return index (1â€‘based) of last nonâ€‘empty row, 0 if fresh."""
    str_vals = ws.col_values(1)
    return len(str_vals)


@backoff.on_exception(backoff.expo, (gspread.exceptions.APIError,), max_tries=5)
def _append_rows(ws, rows: List[list]):
    """Batchâ€‘append rows to worksheet with retry backâ€‘off."""
    # gspread's append_rows uses Sheets API's batchUpdate; this is fast.
    ws.append_rows(rows, value_input_option="USER_ENTERED")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EQUITYÂ TRACKER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def update_equity_tracker(json_path: Path = PORTFOLIO_FILE):
    if not json_path.exists():
        raise FileNotFoundError(json_path)

    with open(json_path) as f:
        values = json.load(f)  # list[float]

    ws = _ensure_worksheet(EQUITY_SHEET)
    start_idx = _last_row(ws)

    header = ["Date", "Equity", "DailyÂ PnL", "CumÂ PnL"]
    if start_idx == 0:
        _append_rows(ws, [header])
        start_idx = 1

    # build rows to append
    rows = []
    init_equity = values[0]
    prev_equity = init_equity if start_idx <= 1 else float(ws.cell(start_idx, 2).value)

    for i, val in enumerate(values[start_idx - 1 :], start=start_idx - 1):
        date = (dt.date.today() - dt.timedelta(days=len(values) - 1 - i)).isoformat()
        daily_pnl = val - prev_equity
        cum_pnl = val - init_equity
        rows.append([date, f"{val:.2f}", f"{daily_pnl:.2f}", f"{cum_pnl:.2f}"])
        prev_equity = val

    if rows:
        # split into â‰¤500â€‘row batches (API limit guard)
        for j in range(0, len(rows), 500):
            _append_rows(ws, rows[j : j + 500])
        print(f"âœ” Added {len(rows)} equity rows (through {rows[-1][0]}).")
    else:
        print("â„¹ Equity sheet already upâ€‘toâ€‘date.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MAIN (CLI) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    update_equity_tracker()
    # To log trades, write a similar function append_trades(trade_df)
