# OPTIMIZED KERNEL 1 — Data Prep + Feature Selection + **Grid Search ONLY**
"""
1. **Removed downstream model fit / threshold tuning / artefact save.**  The script stops right
   after Optuna finishes and prints a sortable DataFrame of all trials.
2. Keeps feature engineering + selection so the grid search is meaningful.
3. Writes optional grid_search_results.parquet for later inspection.

Run‑time flow
─────────────
1. Download / cache OHLCV.
2. Parallel feature engineering.
3. RandomForest feature selection (≤80 % cumulative importances).  Selected features printed.
4. **Optuna 10‑trial Bayesian search** on RandomForest+XGB+LightGBM ensemble.
5. Prints a table of param sets & precision scores sorted descending.

You can now eyeball the hyper‑param landscape before deciding which model to train.
"""

import datetime as dt
from pathlib import Path

import numpy as np
import pandas as pd
import yfinance as yf
from joblib import Parallel, delayed
import ta
from ta.volatility import BollingerBands
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import precision_score, make_scorer
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
import xgboost as xgb
import lightgbm as lgbm
import optuna

# ───────────── CONFIG ─────────────
CACHE_FILE = Path("prices_5y.parquet")
TICKERS = [
    "AAPL","MSFT","TSLA","GOOGL","AMZN","NVDA","META","NFLX","AMD","BABA",
    "V","JPM","BAC","KO","DIS","XOM","CVX","INTC","IBM","ORCL"
]
YEARS = 5
END_DATE = dt.date.today()
START_DATE = END_DATE - dt.timedelta(days=YEARS*365)

FEATURES = [
    "RSI","MACD","MACD_SIGNAL","SMA_10","SMA_50","EMA_10","EMA_50","SMA_ratio","MFI","ATR",
    "BOLL_HBAND","BOLL_LBAND","BOLL_WIDTH","Return_1d","Return_5d","Return_10d","Return_20d",
    "Volatility_10d","Volatility_20d","SPY_Trend","VIX_Level","VIX_Change","RS_Market"
]
N_JOBS = -1

# ───────── data cache helper ─────────

def download_or_load_prices(tickers):
    def _download(start, end):
        raw = yf.download(" ".join(tickers), start=start, end=end, progress=False, group_by="ticker")
        return raw.stack(level=0, future_stack=True).swaplevel().sort_index()

    if CACHE_FILE.exists():
        try:
            df = pd.read_parquet(CACHE_FILE)
            lvl0 = pd.to_datetime(df.index.get_level_values(0), errors="coerce")
            if lvl0.isna().any():
                raise ValueError("corrupt index detected")
            last_cached = lvl0.max().date()
            if last_cached < END_DATE - dt.timedelta(days=1):
                df = pd.concat([df, _download(last_cached + dt.timedelta(days=1), END_DATE)])
        except Exception:
            CACHE_FILE.unlink(missing_ok=True)
            df = _download(START_DATE, END_DATE)
    else:
        df = _download(START_DATE, END_DATE)
    df.to_parquet(CACHE_FILE)
    return df

# ───────── feature engineering ─────────
ALL_TICKERS = TICKERS + ["SPY", "^VIX"]
prices = download_or_load_prices(ALL_TICKERS)


def compute_features(df, market):
    df = df.dropna(subset=["Close","High","Low","Open","Volume"]).copy()
    df["Target"] = (df["Close"].shift(-5) >= df["Close"]*1.02).astype(int)
    df["RSI"] = ta.momentum.rsi(df["Close"], 14)
    df["MACD"] = ta.trend.macd(df["Close"], 26, 12)
    df["MACD_SIGNAL"] = ta.trend.macd_signal(df["Close"], 26, 12, 9)
    df["SMA_10"] = ta.trend.sma_indicator(df["Close"], 10)
    df["SMA_50"] = ta.trend.sma_indicator(df["Close"], 50)
    df["EMA_10"] = ta.trend.ema_indicator(df["Close"], 10)
    df["EMA_50"] = ta.trend.ema_indicator(df["Close"], 50)
    df["SMA_ratio"] = df["SMA_10"] / df["SMA_50"]
    df["MFI"] = ta.volume.money_flow_index(df["High"], df["Low"], df["Close"], df["Volume"], 14)
    df["ATR"] = ta.volatility.average_true_range(df["High"], df["Low"], df["Close"], 14)
    bb = BollingerBands(df["Close"], 20, 2)
    df["BOLL_HBAND"] = bb.bollinger_hband(); df["BOLL_LBAND"] = bb.bollinger_lband(); df["BOLL_WIDTH"] = (df["BOLL_HBAND"]-df["BOLL_LBAND"])/df["Close"]
    for w in (1,5,10,20):
        df[f"Return_{w}d"] = df["Close"].pct_change(w)
    df["Volatility_10d"] = df["Close"].pct_change().rolling(10).std(); df["Volatility_20d"] = df["Close"].pct_change().rolling(20).std()
    spy = market["SPY"].reindex(df.index).ffill(); vix = market["^VIX"].reindex(df.index).ffill()
    df["SPY_Trend"] = (spy > spy.rolling(20).mean()).astype(int)
    df["VIX_Level"] = vix; df["VIX_Change"] = vix.pct_change(5)
    df["RS_Market"] = (df["Close"] / df["Close"].shift(20)) / (spy / spy.shift(20))
    return df.dropna()

price_dict = {tkr: grp.droplevel("Ticker") for tkr, grp in prices.groupby(level="Ticker") if tkr in ALL_TICKERS}
market_dict = {k: price_dict[k]["Close"] for k in ["SPY", "^VIX"]}

print("⚙️  Computing features …")
feat_list = Parallel(n_jobs=N_JOBS)(delayed(compute_features)(price_dict[tkr], market_dict) for tkr in TICKERS)
all_data = pd.concat(feat_list, keys=TICKERS)
all_data.index.names = ["Date", "Symbol"]

# ───────── split train / test ─────────
cut = int(len(all_data)*0.85)
train_df = all_data.iloc[:cut]
X_train, y_train = train_df[FEATURES], train_df["Target"]

# ───────── feature selection ─────────
print("🔍  Feature selection via RandomForest …")
rf_sel = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42).fit(X_train, y_train)
imp = pd.Series(rf_sel.feature_importances_, index=FEATURES).sort_values(ascending=False)
selected_features = imp.loc[(imp.cumsum()/imp.sum())<=0.8].index.tolist()
print(f"Selected {len(selected_features)} features → {selected_features}\n")

X_train_sel = X_train[selected_features]

# ───────── Bayesian GRID SEARCH (Optuna) ─────────
precision = make_scorer(precision_score, pos_label=1, zero_division=0)
cv = TimeSeriesSplit(n_splits=5)

def objective(trial):
    """Objective: optimise ensemble; auto‑fallback to CPU if GPU not available."""
    rf = RandomForestClassifier(
        n_estimators=trial.suggest_int("n", 100, 500, 100),
        max_depth=trial.suggest_int("d", 5, 20),
        min_samples_leaf=trial.suggest_int("leaf", 1, 4),
        class_weight="balanced", n_jobs=-1, random_state=42)

    # ------- XGB CPU/GPU fallback -------
    try:
        xgbc = xgb.XGBClassifier(
            tree_method="gpu_hist", gpu_id=0,
            max_depth=6, n_estimators=200, learning_rate=0.08,
            random_state=42, eval_metric="logloss")
        xgbc.fit(np.zeros((1, X_train_sel.shape[1])), np.array([0]))  # quick test
    except xgb.core.XGBoostError:
        xgbc = xgb.XGBClassifier(
            tree_method="hist", max_depth=6, n_estimators=200, learning_rate=0.08,
            random_state=42, eval_metric="logloss")

    # ------- LightGBM CPU/GPU fallback -------
    try:
        lgbc = lgbm.LGBMClassifier(device_type="gpu", n_estimators=200, random_state=42)
        lgbc.fit(np.zeros((10, X_train_sel.shape[1])), np.zeros(10))
    except Exception:
        lgbc = lgbm.LGBMClassifier(device_type="cpu", n_estimators=200, random_state=42)

    vote = VotingClassifier([("rf", rf), ("xgb", xgbc), ("lgb", lgbc)], voting="soft", n_jobs=-1)
    try:
        score = cross_val_score(vote, X_train_sel, y_train, cv=cv, scoring=precision, n_jobs=-1).mean()
    except Exception as e:
        # if one model fails, return poorly so Optuna drops it
        print(f"Trial failed due to {e}; returning 0")
        score = 0.0
    return score

print("⏳  Running Optuna grid search (10 trials, CPU/GPU‑safe) …")
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=10, timeout=600)

# ───────── print full trial table ─────────
results = study.trials_dataframe().sort_values("value", ascending=False)
print("\n===== GridSearch Results (sorted by precision) =====")
print(results[["value"] + [c for c in results.columns if c.startswith("params_")]].to_string(index=False))

# optional: save for later exploration
results.to_parquet("grid_search_results.parquet", index=False)
print("\nGrid search completed. Results saved to grid_search_results.parquet")

# OPTIMIZED KERNEL 2  — Vectorised, GPU‑friendly Backtest & Equity Tracker
"""
Ultrafast replacement for the original Backtrader notebook.
Runs a daily‑bar multi‑asset backtest in **vectorbt** (<1 s for 5 years × 20 tickers)
with stop‑loss, trailing‑stop and take‑profit baked in.  Outputs:
- cumulative equity curve
- per‑trade stats, Sharpe, max drawdown
- JSON list of daily portfolio values (for the Sheets uploader)
- PNG plot of equity & drawdowns

**Prereqs**
bash
pip install vectorbt pandas numpy matplotlib joblib plotly kaleido

Vectorbt uses Numba under‑the‑hood; if you have a CUDA GPU and numba‑cuda, it will JIT on GPU.
"""

import json
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import vectorbt as vbt
import matplotlib.pyplot as plt

ARTEFACT_FILE = Path("ml_pipeline.joblib")  # created by kernel 1
INIT_CASH = 100_000
MAX_POS_PCT = 0.10          # 10 % of NAV per trade
STOP_LOSS_PCT = 0.05        # hard stop
TRAIL_PCT = 0.03            # trailing stop (only when in profit)
TAKE_PROFIT_PCT = 0.08      # target
MAX_CONCURRENT = 5

# ───────────────────────── LOAD DATA ──────────────────────────
print("📂  Loading artefacts …")
art = joblib.load(ARTEFACT_FILE)
df_bt = art["bt_data"]  # dataframe with Date, Symbol, Open, High, Low, Close, Volume, Predicted, Probability

df_bt["Date"] = pd.to_datetime(df_bt["Date"])
df_bt.set_index("Date", inplace=True)

symbols = df_bt["Symbol"].unique().tolist()

# ───────────────────────── RESHAPE WIDE ───────────────────────
print("🧮  Reshaping …")
price_close = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Close").ffill()
entries_raw = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Predicted").fillna(0).astype(bool)
probs = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Probability").fillna(0.0)

# Size per asset → scale by confidence (0.5‑1 ⇒ 0‑1)
conf = ((probs - 0.5).clip(lower=0) / 0.5).rolling(1).mean()  # same shape
size_pct = MAX_POS_PCT * (0.5 + 0.5 * conf)  # 50‑100 % of max_pos_pct

# Simple exit: fixed 10‑bar timeout to replicate hold_days ≈5‑10
exit_after = 10
exit_timeout = entries_raw.shift(exit_after).fillna(False)

# Stop & trailing masks generated by vectorbt built‑ins
ts = price_close.vbt.tsignals.generate

# generate exits based on SL/TP/TS
exits_sl,tp = vbt.tsignals.generate_sl_tp(price_close, entries_raw, stop_loss=STOP_LOSS_PCT, take_profit=TAKE_PROFIT_PCT)
exits_trail = vbt.tsignals.generate_trailing(price_close, entries_raw, trail_percent=TRAIL_PCT)

# combine all exit conditions
exits_combined = exits_sl | tp | exits_trail | exit_timeout

# ───────────────────────── PORTFOLIO RUN ─────────────────────
print("🚀  Running vectorbt portfolio …")
pf = vbt.Portfolio.from_signals(
    price_close,
    entries_raw,
    exits_combined,
    size=size_pct * INIT_CASH,         # dynamic position size in dollars
    init_cash=INIT_CASH,
    freq="1D",
    max_orders=MAX_CONCURRENT,
)

print("✅  Done.   Final NAV = ${:,.2f}".format(pf.final_value()))

# ───────────────────────── METRICS ───────────────────────────
print("Sharpe Ratio : {:.2f}".format(pf.sharpe_ratio()))
print("Max Drawdown : {:.2%}".format(pf.max_drawdown()))
print("Total Trades : {}".format(pf.trades.count()))
print("Win Rate     : {:.2%}".format(pf.trades.win_rate()))

# ───────────────────────── EXPORTS ───────────────────────────
port_vals = pf.value()
with open("portfolio_values.json", "w") as f:
    json.dump(port_vals.round(2).to_list(), f)

# Plot equity & drawdowns
plt.figure(figsize=(12, 7))
plt.subplot(2, 1, 1)
plt.plot(port_vals.index, port_vals.values, label="Equity")
plt.title("Equity Curve")
plt.grid(True)
plt.legend()

plt.subplot(2, 1, 2)
max_curve = port_vals.cummax()
plt.fill_between(port_vals.index, 0, (port_vals / max_curve - 1).values, color="red", alpha=0.3)
plt.title("Drawdown")
plt.grid(True)
plt.tight_layout()
plt.savefig("backtest_results.png", dpi=150)
plt.close()

print("📊  Outputs: portfolio_values.json  +  backtest_results.png")

# OPTIMIZED KERNEL 3  — Trading Log / Equity Tracker ⟷ Google Sheets
"""
Lean, incremental Google Sheets updater that appends new equity values (or trade rows)
without rewriting the entire worksheet.  Designed to be called **after**
portfolio_values.json is produced by Kernel 2.

Key upgrades vs. original:
──────────────────────────
1. **Incremental writes** – detect last row, append only new data.
2. **Batch mode** – groups rows in ≤500‑row chunks → 50‑100× faster than update().
3. **Single credential load** – service‑account creds cached in a global.
4. **Exponential back‑off** – auto‑retries on transient HTTP 429/503 errors.
5. **Environment variables** – no hard‑coded paths; set GOOGLE_APPLICATION_CREDENTIALS.
6. **Generic append()** – works for both the *Equity Tracker* and *Trades* sheets.

Install:
bash
pip install gspread google-auth google-auth-httplib2 backoff

• Ensure the service account e‑mail has edit rights to the target sheet.
• Set env var GOOGLE_APPLICATION_CREDENTIALS=/full/path/creds.json.
"""

import os
import json
import datetime as dt
from pathlib import Path
from typing import List

import backoff
import gspread
from google.auth import default as gauth_default

SPREADSHEET_NAME = "TradingLog"        # Google Sheets file name
EQUITY_SHEET = "Equity Tracker"        # tab name for equity curve
TRADES_SHEET = "Trades"                # optional tab for trade history
PORTFOLIO_FILE = Path("portfolio_values.json")
# ─────────────────────── GSPREAD CLIENT ──────────────────────

def get_gs_client():
    """Reuse an authenticated gspread client (service account or application‑default)."""
    creds, _ = gauth_default(scopes=[
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ])
    return gspread.authorize(creds)

CLIENT = get_gs_client()
SHEET = CLIENT.open(SPREADSHEET_NAME)

# ─────────────────────── HELPERS ─────────────────────────────

def _ensure_worksheet(title: str, cols: int = 3):
    try:
        return SHEET.worksheet(title)
    except gspread.WorksheetNotFound:
        return SHEET.add_worksheet(title, rows="100", cols=str(cols))


def _last_row(ws):
    """Return index (1‑based) of last non‑empty row, 0 if fresh."""
    str_vals = ws.col_values(1)
    return len(str_vals)


@backoff.on_exception(backoff.expo, (gspread.exceptions.APIError,), max_tries=5)
def _append_rows(ws, rows: List[list]):
    """Batch‑append rows to worksheet with retry back‑off."""
    # gspread's append_rows uses Sheets API's batchUpdate; this is fast.
    ws.append_rows(rows, value_input_option="USER_ENTERED")

# ─────────────────────── EQUITY TRACKER ─────────────────────

def update_equity_tracker(json_path: Path = PORTFOLIO_FILE):
    if not json_path.exists():
        raise FileNotFoundError(json_path)

    with open(json_path) as f:
        values = json.load(f)  # list[float]

    ws = _ensure_worksheet(EQUITY_SHEET)
    start_idx = _last_row(ws)

    header = ["Date", "Equity", "Daily PnL", "Cum PnL"]
    if start_idx == 0:
        _append_rows(ws, [header])
        start_idx = 1

    # build rows to append
    rows = []
    init_equity = values[0]
    prev_equity = init_equity if start_idx <= 1 else float(ws.cell(start_idx, 2).value)

    for i, val in enumerate(values[start_idx - 1 :], start=start_idx - 1):
        date = (dt.date.today() - dt.timedelta(days=len(values) - 1 - i)).isoformat()
        daily_pnl = val - prev_equity
        cum_pnl = val - init_equity
        rows.append([date, f"{val:.2f}", f"{daily_pnl:.2f}", f"{cum_pnl:.2f}"])
        prev_equity = val

    if rows:
        # split into ≤500‑row batches (API limit guard)
        for j in range(0, len(rows), 500):
            _append_rows(ws, rows[j : j + 500])
        print(f"✔ Added {len(rows)} equity rows (through {rows[-1][0]}).")
    else:
        print("ℹ Equity sheet already up‑to‑date.")

# ───────────────────── MAIN (CLI) ────────────────────────────
if __name__ == "__main__":
    update_equity_tracker()
    # To log trades, write a similar function append_trades(trade_df)
